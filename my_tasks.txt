AMREE:
run.py
--task_name
AMREE
--do_train
--data_dir
DeBERTa/amree_data
--eval_batch_size
8
--predict_batch_size
8
--output_dir
DeBERTa/amree-model
--num_train_epochs
10
--warmup
100
--learning_rate
1e-5
--train_batch_size
4
--max_seq_len
350
--init_model
./deberta-large/pytorch_model.bin
--model_config
./deberta-large/model_config.json
--vocab_type
large
--workers
0
--dump_interval
500
--collate_pad
--no_batch_to

MLGM:
--task_name
MLGM
--do_train
--data_dir
DeBERTa/mlgm-amr3
--eval_batch_size
8
--predict_batch_size
8
--output_dir
DeBERTa/mlgm-model
--num_train_epochs
10
--warmup
100
--learning_rate
1e-5
--train_batch_size
4
--max_seq_len
350
--init_model
./deberta-large/pytorch_model.bin
--model_config
./deberta-large/model_config.json
--vocab_type
large
--workers
0
--dump_interval
3000